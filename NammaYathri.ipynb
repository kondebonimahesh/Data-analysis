{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. Conversion Funnel Deep Dive\n",
        "â€‹Compute conversion rates across funnel stages â€‹(Search â†’ Quote â†’ Booking â†’ Completed). Segment by time-of-day Morning (6â€“10 AM), Day (10 AMâ€“5 PM), Evening (5â€“9 PM), Night (9 PMâ€“6 AM) and trip length (short <5 km, medium 5â€“15 km, long >15 km), Identify the maximum drop within each segment. Propose hypotheses for observed drops and validate with data"
      ],
      "metadata": {
        "id": "tcPIYO8SCe-z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "8Li5Q5n_CLxX",
        "outputId": "0d95498c-7697-4f7c-aa00-cea36cbecf7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/My Drive/NY Datathon 2025/NY Datathon 2025' not found.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1155782355.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcsv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcsv_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pytz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "folder_path = '/content/drive/My Drive/NY Datathon 2025/NY Datathon 2025'\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f\"Files in '{folder_path}':\")\n",
        "    for file in files:\n",
        "        print(file)\n",
        "else:\n",
        "    print(f\"Folder '{folder_path}' not found.\")\n",
        "\n",
        "csv_files = []\n",
        "for file in files:\n",
        "    if file.endswith('.csv'):\n",
        "        csv_files.append(file)\n",
        "\n",
        "print(\"CSV files found:\")\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)\n",
        "\n",
        "dfs = {}\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    file_path = os.path.join(folder_path, csv_file)\n",
        "    df_name = os.path.splitext(csv_file)[0]\n",
        "    dfs[df_name] = pd.read_csv(file_path)\n",
        "\n",
        "for df_name, df in dfs.items():\n",
        "    print(f\"DataFrame '{df_name}':\")\n",
        "    display(df.head())\n",
        "\n",
        "def convert_utc_to_ist(utc_timestamp):\n",
        "\n",
        "    if pd.isna(utc_timestamp):\n",
        "        return None\n",
        "    try:\n",
        "        utc_time = pd.to_datetime(utc_timestamp, utc=True)\n",
        "        ist_time = utc_time.tz_convert('Asia/Kolkata')\n",
        "        return ist_time\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting timestamp {utc_timestamp}: {e}\")\n",
        "        return None\n",
        "\n",
        "for df_name, time_col in [\n",
        "    ('search_data', 'search_request_created_at'),\n",
        "    ('quote_data', 'quote_created_at'),\n",
        "    ('booking_data', 'booking_created_at'),\n",
        "    ('booking_cancellation_data', 'cancelled_at')\n",
        "]:\n",
        "    dfs[df_name][f'{time_col}_ist'] = dfs[df_name][time_col].apply(convert_utc_to_ist)\n",
        "\n",
        "\n",
        "for df_name, time_col in [\n",
        "    ('search_data', 'search_request_created_at'),\n",
        "    ('quote_data', 'quote_created_at'),\n",
        "    ('booking_data', 'booking_created_at')\n",
        "]:\n",
        "    dfs[df_name][time_col] = pd.to_datetime(dfs[df_name][time_col])\n",
        "\n",
        "merged_df = (\n",
        "    dfs['search_data']\n",
        "    .merge(dfs['quote_data'], on='search_request_id', how='left')\n",
        "    .merge(dfs['booking_data'], on='quote_id', how='left')\n",
        "    .merge(dfs['booking_cancellation_data'], on='booking_id', how='left')\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "merged_df['hour_of_day'] = merged_df['search_request_created_at'].dt.hour\n",
        "\n",
        "def get_time_of_day_segment(hour):\n",
        "    if 4 <= hour < 11:\n",
        "        return 'Morning'\n",
        "    elif 11 <= hour < 16:\n",
        "        return 'Day'\n",
        "    elif 16 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "merged_df['time_of_day_segment'] = merged_df['hour_of_day'].apply(get_time_of_day_segment)\n",
        "\n",
        "\n",
        "def categorize_trip_length(distance):\n",
        "    if pd.isna(distance):\n",
        "        return 'unknown'\n",
        "    elif distance < 5000:\n",
        "        return 'short'\n",
        "    elif 5000 <= distance < 20000:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'long'\n",
        "\n",
        "merged_df['trip_length_segment'] = merged_df['estimated_distance'].apply(categorize_trip_length)\n",
        "print(\"\\n trip length distribution:\")\n",
        "display(merged_df['trip_length_segment'].value_counts())\n",
        "\n",
        "\n",
        "funnel_stages = ['search_request_id', 'quote_id', 'booking_id', 'status']\n",
        "\n",
        "funnel_counts = (\n",
        "    merged_df.groupby(['time_of_day_segment', 'trip_length_segment'])[funnel_stages]\n",
        "    .count()\n",
        "    .rename(columns={\n",
        "        'search_request_id': 'Search',\n",
        "        'quote_id': 'Quote',\n",
        "        'booking_id': 'Booking',\n",
        "        'status': 'Completed'\n",
        "    })\n",
        ")\n",
        "\n",
        "completed_counts = (\n",
        "    merged_df[merged_df['status'] == 'COMPLETED']\n",
        "    .groupby(['time_of_day_segment', 'trip_length_segment'])['booking_id']\n",
        "    .count()\n",
        ")\n",
        "\n",
        "funnel_counts['Completed'] = completed_counts.fillna(0).astype(int)\n",
        "\n",
        "funnel_counts['Search_to_Quote_Conversion'] = (funnel_counts['Quote'] / funnel_counts['Search']) * 100\n",
        "funnel_counts['Quote_to_Booking_Conversion'] = (funnel_counts['Booking'] / funnel_counts['Quote']) * 100\n",
        "funnel_counts['Booking_to_Completed_Conversion'] = (funnel_counts['Completed'] / funnel_counts['Booking']) * 100\n",
        "\n",
        "print(\"\\nðŸ“Š Funnel conversion rates per segment:\")\n",
        "display(funnel_counts)\n",
        "\n",
        "\n",
        "max_drop_per_segment = {}\n",
        "for index, row in funnel_counts.iterrows():\n",
        "    conversions = {\n",
        "        'Search_to_Quote_Conversion': row['Search_to_Quote_Conversion'],\n",
        "        'Quote_to_Booking_Conversion': row['Quote_to_Booking_Conversion'],\n",
        "        'Booking_to_Completed_Conversion': row['Booking_to_Completed_Conversion']\n",
        "    }\n",
        "    max_drop_stage = min(conversions, key=conversions.get)\n",
        "    max_drop_per_segment[index] = {\n",
        "        'Stage': max_drop_stage,\n",
        "        'Max_Drop_Percentage': conversions[max_drop_stage]\n",
        "    }\n",
        "\n",
        "max_drop_df = pd.DataFrame.from_dict(max_drop_per_segment, orient='index')\n",
        "print(\"\\nðŸ”» Maximum conversion drop per segment:\")\n",
        "display(max_drop_df)\n",
        "\n",
        "\n",
        "print(\"\\n Estimated Distance & Duration vs Quote Analysis:\")\n",
        "\n",
        "\n",
        "merged_df['got_quote'] = merged_df['quote_id'].notna()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x='got_quote', y='estimated_distance', data=merged_df)\n",
        "plt.title('Estimated Distance vs. Got Quote')\n",
        "plt.xlabel('Got Quote')\n",
        "plt.ylabel('Estimated Distance')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x='got_quote', y='estimated_duration', data=merged_df)\n",
        "plt.title('Estimated Duration vs. Got Quote')\n",
        "plt.xlabel('Got Quote')\n",
        "plt.ylabel('Estimated Duration')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "quoted_df = merged_df[merged_df['got_quote'] == True].copy()\n",
        "\n",
        "print(\"\\nAn alysis of Fare and Pickup Metrics for Quoted Searches:\")\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.boxplot(x='time_of_day_segment', y='estimated_fare', data=quoted_df)\n",
        "plt.title('Estimated Fare by Time of Day')\n",
        "plt.xlabel('Time of Day')\n",
        "plt.ylabel('Estimated Fare')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.kdeplot(quoted_df['distance_to_pickup'], fill=True)\n",
        "plt.title('Distance to Pickup Distribution')\n",
        "plt.xlabel('Distance to Pickup')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.kdeplot(quoted_df['duration_to_pickup'], fill=True)\n",
        "plt.title('Duration to Pickup Distribution')\n",
        "plt.xlabel('Duration to Pickup')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n  Final Funnel Conversion Summary:\")\n",
        "display(funnel_counts)\n",
        "\n",
        "print(\"\\n Maximum Conversion Drop Summary:\")\n",
        "display(max_drop_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. Booking Cancellations\n",
        "Compute the overall cancellation rate. Break down by driver vs rider cancellations, pickup distance buckets and trip distance buckets. Identify the top 3 driver cancellation reasons and analyze their trends over time-of-day and trip length. Analyze the relationship between driver ratings and driver cancellations, and provide key insightsâ€‹"
      ],
      "metadata": {
        "id": "qTg2M9mtC16n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "folder_path = '/content/drive/My Drive/NY Datathon 2025/NY Datathon 2025'\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f\"Files in '{folder_path}':\")\n",
        "    for file in files:\n",
        "        print(file)\n",
        "else:\n",
        "    print(f\"Folder '{folder_path}' not found.\")\n",
        "\n",
        "\n",
        "csv_files = []\n",
        "for file in files:\n",
        "    if file.endswith('.csv'):\n",
        "        csv_files.append(file)\n",
        "\n",
        "print(\"CSV files found:\")\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dfs = {}\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    file_path = os.path.join(folder_path, csv_file)\n",
        "    df_name = os.path.splitext(csv_file)[0]\n",
        "    dfs[df_name] = pd.read_csv(file_path)\n",
        "\n",
        "for df_name, df in dfs.items():\n",
        "    print(f\"DataFrame '{df_name}':\")\n",
        "    display(df.head())\n",
        "  import pandas as pd\n",
        "\n",
        "def convert_utc_to_ist(utc_timestamp):\n",
        "\n",
        "    if pd.isna(utc_timestamp):\n",
        "        return None\n",
        "    try:\n",
        "        utc_time = pd.to_datetime(utc_timestamp, utc=True)\n",
        "        ist_time = utc_time.tz_convert('Asia/Kolkata')\n",
        "        return ist_time\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting timestamp {utc_timestamp}: {e}\")\n",
        "        return None\n",
        "\n",
        "dfs['search_data']['search_request_created_at_ist'] = dfs['search_data']['search_request_created_at'].apply(convert_utc_to_ist)\n",
        "dfs['quote_data']['quote_created_at_ist'] = dfs['quote_data']['quote_created_at'].apply(convert_utc_to_ist)\n",
        "dfs['booking_data']['booking_created_at_ist'] = dfs['booking_data']['booking_created_at'].apply(convert_utc_to_ist)\n",
        "dfs['booking_cancellation_data']['cancelled_at_ist'] = dfs['booking_cancellation_data']['cancelled_at'].apply(convert_utc_to_ist)\n",
        "\n",
        "\n",
        "print(\"DataFrame 'search_data' with IST timestamp:\")\n",
        "display(dfs['search_data'].head())\n",
        "\n",
        "print(\"DataFrame 'quote_data' with IST timestamp:\")\n",
        "display(dfs['quote_data'].head())\n",
        "\n",
        "print(\"DataFrame 'booking_data' with IST timestamp:\")\n",
        "display(dfs['booking_data'].head())\n",
        "\n",
        "print(\"DataFrame 'booking_cancellation_data' with IST timestamp:\")\n",
        "display(dfs['booking_cancellation_data'].head())\n",
        "total_bookings = dfs['booking_data'].shape[0]\n",
        "total_cancellations = dfs['booking_cancellation_data'].shape[0]\n",
        "\n",
        "print(f\"Total number of bookings: {total_bookings}\")\n",
        "print(f\"Total number of cancellations: {total_cancellations}\")\n",
        "print(\"Columns in dfs['search_data']:\")\n",
        "print(dfs['search_data'].columns)\n",
        "\n",
        "cancelled_bookings_with_quote = pd.merge(merged_cancellations, dfs['quote_data'], on='quote_id', how='left')\n",
        "\n",
        "cancelled_bookings_with_search = pd.merge(cancelled_bookings_with_quote, dfs['search_data'], on='search_request_id', how='left')\n",
        "\n",
        "trip_distance_bins = [0, 5000, 10000, 25000, 50000, 100000, float('inf')]\n",
        "trip_distance_labels = ['0-5km', '5-10km', '10-25km', '25-50km', '50-100km', '>100km']\n",
        "\n",
        "cancelled_bookings_with_search['trip_distance_bucket'] = pd.cut(cancelled_bookings_with_search['estimated_distance'], bins=trip_distance_bins, labels=trip_distance_labels, right=False)\n",
        "\n",
        "cancellations_by_trip_distance = cancelled_bookings_with_search['trip_distance_bucket'].value_counts().reset_index()\n",
        "cancellations_by_trip_distance.columns = ['trip_distance_bucket', 'cancellation_count']\n",
        "\n",
        "all_bookings_with_quote = pd.merge(dfs['booking_data'], dfs['quote_data'], on='quote_id', how='left')\n",
        "all_bookings_with_search = pd.merge(all_bookings_with_quote, dfs['search_data'], on='search_request_id', how='left')\n",
        "all_bookings_with_search['trip_distance_bucket'] = pd.cut(all_bookings_with_search['estimated_distance'], bins=trip_distance_bins, labels=trip_distance_labels, right=False)\n",
        "total_bookings_by_trip_distance = all_bookings_with_search['trip_distance_bucket'].value_counts().reset_index()\n",
        "total_bookings_by_trip_distance.columns = ['trip_distance_bucket', 'total_booking_count']\n",
        "\n",
        "trip_cancellation_rates = pd.merge(cancellations_by_trip_distance, total_bookings_by_trip_distance, on='trip_distance_bucket', how='left')\n",
        "\n",
        "trip_cancellation_rates['cancellation_rate'] = (trip_cancellation_rates['cancellation_count'] / trip_cancellation_rates['total_booking_count']) * 100\n",
        "\n",
        "print(\"\\nCancellation rate by trip distance bucket:\")\n",
        "display(trip_cancellation_rates.sort_values('trip_distance_bucket'))\n",
        "driver_cancellations = merged_cancellations[merged_cancellations['source'] == 'ByDriver']\n",
        "\n",
        "driver_cancellation_reasons = driver_cancellations['reason_code'].value_counts()\n",
        "\n",
        "top_3_driver_cancellation_reasons = driver_cancellation_reasons.head(3)\n",
        "\n",
        "print(\"Top 3 Driver Cancellation Reasons:\")\n",
        "display(top_3_driver_cancellation_reasons)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_reasons = top_3_driver_cancellation_reasons.index.tolist()\n",
        "filtered_cancellations = merged_cancellations[merged_cancellations['reason_code'].isin(top_reasons)]\n",
        "filtered_cancellations['cancelled_at'] = pd.to_datetime(filtered_cancellations['cancelled_at'])\n",
        "\n",
        "filtered_cancellations['cancellation_hour'] = filtered_cancellations['cancelled_at'].dt.hour\n",
        "\n",
        "cancellation_trends = filtered_cancellations.groupby(['cancellation_hour', 'reason_code']).size().unstack(fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for reason in top_reasons:\n",
        "    plt.plot(cancellation_trends.index, cancellation_trends[reason], marker='o', linestyle='-', label=reason)\n",
        "\n",
        "plt.title('Trends of Top 3 Driver Cancellation Reasons Over Time of Day')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Cancellations')\n",
        "plt.xticks(range(24))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "driver_cancellations_with_quote = pd.merge(driver_cancellations, dfs['quote_data'], on='quote_id', how='left')\n",
        "\n",
        "driver_cancellations_with_search = pd.merge(driver_cancellations_with_quote, dfs['search_data'], on='search_request_id', how='left')\n",
        "\n",
        "trip_distance_bins = [0, 5000, 10000, 25000, 50000, 100000, float('inf')]\n",
        "trip_distance_labels = ['0-5km', '5-10km', '10-25km', '25-50km', '50-100km', '>100km']\n",
        "\n",
        "driver_cancellations_with_search['trip_distance_bucket'] = pd.cut(driver_cancellations_with_search['estimated_distance'], bins=trip_distance_bins, labels=trip_distance_labels, right=False)\n",
        "\n",
        "top_reasons = top_3_driver_cancellation_reasons.index.tolist()\n",
        "filtered_driver_cancellations = driver_cancellations_with_search[driver_cancellations_with_search['reason_code'].isin(top_reasons)]\n",
        "\n",
        "cancellation_trends_by_distance = filtered_driver_cancellations.groupby(['trip_distance_bucket', 'reason_code']).size().unstack(fill_value=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "cancellation_trends_by_distance.plot(kind='bar', figsize=(12, 6))\n",
        "\n",
        "plt.title('Trends of Top 3 Driver Cancellation Reasons by Trip Length')\n",
        "plt.xlabel('Trip Distance Bucket')\n",
        "plt.ylabel('Number of Cancellations')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Reason Code')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Non-numeric values in driver_cancellations_with_rating['driver_rating']:\")\n",
        "display(driver_cancellations_with_rating[pd.to_numeric(driver_cancellations_with_rating['driver_rating'], errors='coerce').isna()]['driver_rating'].unique())\n",
        "\n",
        "print(\"\\nNon-numeric values in dfs['quote_data']['driver_rating']:\")\n",
        "display(dfs['quote_data'][pd.to_numeric(dfs['quote_data']['driver_rating'], errors='coerce').isna()]['driver_rating'].unique())\n",
        "\n",
        "driver_cancellations_with_rating['driver_rating'] = pd.to_numeric(driver_cancellations_with_rating['driver_rating'], errors='coerce')\n",
        "dfs['quote_data']['driver_rating'] = pd.to_numeric(dfs['quote_data']['driver_rating'], errors='coerce')\n",
        "\n",
        "driver_cancellations_with_rating.dropna(subset=['driver_rating'], inplace=True)\n",
        "dfs['quote_data'].dropna(subset=['driver_rating'], inplace=True)\n",
        "\n",
        "rating_bins = [0, 4, 4.5, 4.8, 5.1]\n",
        "rating_labels = ['<4', '4-4.5', '4.5-4.8', '4.8-5']\n",
        "\n",
        "\n",
        "driver_cancellations_with_rating['driver_rating_bucket'] = pd.cut(driver_cancellations_with_rating['driver_rating'], bins=rating_bins, labels=rating_labels, right=False, include_lowest=True)\n",
        "\n",
        "cancellations_by_rating = driver_cancellations_with_rating['driver_rating_bucket'].value_counts().reset_index()\n",
        "cancellations_by_rating.columns = ['driver_rating_bucket', 'cancellation_count']\n",
        "\n",
        "dfs['quote_data']['driver_rating_bucket'] = pd.cut(dfs['quote_data']['driver_rating'], bins=rating_bins, labels=rating_labels, right=False, include_lowest=True)\n",
        "\n",
        "total_quotes_by_rating = dfs['quote_data']['driver_rating_bucket'].value_counts().reset_index()\n",
        "total_quotes_by_rating.columns = ['driver_rating_bucket', 'total_quote_count']\n",
        "\n",
        "rating_cancellation_rates = pd.merge(cancellations_by_rating, total_quotes_by_rating, on='driver_rating_bucket', how='left')\n",
        "\n",
        "rating_cancellation_rates['cancellation_rate'] = (rating_cancellation_rates['cancellation_count'] / rating_cancellation_rates['total_quote_count']) * 100\n",
        "\n",
        "print(\"\\nDriver cancellation rate by driver rating bucket:\")\n",
        "display(rating_cancellation_rates.sort_values('driver_rating_bucket'))"
      ],
      "metadata": {
        "id": "ruILxlouC8BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd question\n",
        "## â€‹Q3. Critical Stage & Root Cause Analysis Identify the most critical funnel stage using insights from Q1 and Q2. Perform clustering on drivers/riders to discover behavioral segments (e.g., high-quote, low-booking drivers). Analyze weekly time-series trends for conversion and cancellations to detect seasonality."
      ],
      "metadata": {
        "id": "S14yAQpiDe-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_search_requests = dfs['search_data']['search_request_id'].nunique()\n",
        "num_quotes = dfs['quote_data']['quote_id'].nunique()\n",
        "num_bookings = dfs['booking_data']['booking_id'].nunique()\n",
        "\n",
        "search_to_quote_rate = (num_quotes / num_search_requests) * 100\n",
        "quote_to_booking_rate = (num_bookings / num_quotes) * 100\n",
        "\n",
        "\n",
        "print(f\"Number of unique search requests: {num_search_requests}\")\n",
        "print(f\"Number of unique quotes: {num_quotes}\")\n",
        "print(f\"Number of unique bookings: {num_bookings}\")\n",
        "print(f\"Conversion rate from Search to Quote: {search_to_quote_rate:.2f}%\")\n",
        "print(f\"Conversion rate from Quote to Booking: {quote_to_booking_rate:.2f}%\")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "driver_features = dfs['quote_data'].groupby('driver_id').agg(\n",
        "    total_quotes=('quote_id', 'count'),\n",
        "    avg_driver_rating=('driver_rating', 'mean'),\n",
        ").reset_index()\n",
        "\n",
        "driver_bookings = dfs['booking_data'].merge(dfs['quote_data'][['quote_id', 'driver_id']], on='quote_id', how='left')\n",
        "driver_booked_quotes_count = driver_bookings.groupby('driver_id').agg(booked_quotes=('booking_id', 'count')).reset_index()\n",
        "\n",
        "driver_cancellations = dfs['booking_cancellation_data'].merge(dfs['booking_data'][['booking_id', 'quote_id']], on='booking_id', how='left')\n",
        "driver_cancelled_quotes_count = driver_cancellations.merge(dfs['quote_data'][['quote_id', 'driver_id']], on='quote_id', how='left').groupby('driver_id').agg(cancelled_quotes=('booking_id', 'count')).reset_index()\n",
        "\n",
        "\n",
        "driver_features = driver_features.merge(driver_booked_quotes_count, on='driver_id', how='left').merge(driver_cancelled_quotes_count, on='driver_id', how='left')\n",
        "driver_features.fillna(0, inplace=True)\n",
        "\n",
        "driver_features['quote_acceptance_rate'] = driver_features['booked_quotes'] / driver_features['total_quotes']\n",
        "driver_features['cancellation_rate'] = driver_features['cancelled_quotes'] / driver_features['booked_quotes']\n",
        "\n",
        "rider_features = dfs['booking_data'].groupby('rider_id').agg(\n",
        "    total_bookings=('booking_id', 'count'),\n",
        ").reset_index()\n",
        "\n",
        "rider_cancellations_count = dfs['booking_cancellation_data'][dfs['booking_cancellation_data']['source'] == 'ByUser'].groupby('booking_id').size().reset_index(name='cancellation_count')\n",
        "rider_cancellations_count = rider_cancellations_count.merge(dfs['booking_data'][['booking_id', 'rider_id']], on='booking_id', how='left')\n",
        "rider_cancelled_bookings_count = rider_cancellations_count.groupby('rider_id').agg(cancelled_bookings=('booking_id', 'count')).reset_index()\n",
        "\n",
        "rider_features = rider_features.merge(rider_cancelled_bookings_count, on='rider_id', how='left')\n",
        "rider_features.fillna(0, inplace=True)\n",
        "\n",
        "rider_features['cancellation_rate'] = rider_features['cancelled_bookings'] / rider_features['total_bookings']\n",
        "\n",
        "\n",
        "driver_clustering_features = driver_features[['total_quotes', 'avg_driver_rating', 'quote_acceptance_rate', 'cancellation_rate']]\n",
        "rider_clustering_features = rider_features[['total_bookings', 'cancellation_rate']]\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "driver_clustering_features_imputed = imputer.fit_transform(driver_clustering_features)\n",
        "rider_clustering_features_imputed = imputer.fit_transform(rider_clustering_features)\n",
        "\n",
        "\n",
        "scaler_driver = StandardScaler()\n",
        "driver_scaled_features = scaler_driver.fit_transform(driver_clustering_features_imputed)\n",
        "\n",
        "scaler_rider = StandardScaler()\n",
        "rider_scaled_features = scaler_rider.fit_transform(rider_clustering_features_imputed)\n",
        "\n",
        "print(\"Driver features for clustering (first 5 rows after scaling and imputation):\")\n",
        "display(pd.DataFrame(driver_scaled_features, columns=driver_clustering_features.columns).head())\n",
        "\n",
        "print(\"\\nRider features for clustering (first 5 rows after scaling and imputation):\")\n",
        "display(pd.DataFrame(rider_scaled_features, columns=rider_clustering_features.columns).head())\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inertia_driver = []\n",
        "k_range_driver = range(1, 11)\n",
        "for k in k_range_driver:\n",
        "    kmeans_driver = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_driver.fit(driver_scaled_features)\n",
        "    inertia_driver.append(kmeans_driver.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_range_driver, inertia_driver, marker='o')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Driver Clustering')\n",
        "plt.xticks(k_range_driver)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "n_clusters_driver = 3\n",
        "kmeans_driver = KMeans(n_clusters=n_clusters_driver, random_state=42, n_init=10)\n",
        "driver_features['driver_cluster'] = kmeans_driver.fit_predict(driver_scaled_features)\n",
        "\n",
        "inertia_rider = []\n",
        "k_range_rider = range(1, 11)\n",
        "for k in k_range_rider:\n",
        "    kmeans_rider = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_rider.fit(rider_scaled_features)\n",
        "    inertia_rider.append(kmeans_rider.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_range_rider, inertia_rider, marker='o')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Rider Clustering')\n",
        "plt.xticks(k_range_rider)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "n_clusters_rider = 3\n",
        "kmeans_rider = KMeans(n_clusters=n_clusters_rider, random_state=42, n_init=10)\n",
        "rider_features['rider_cluster'] = kmeans_rider.fit_predict(rider_scaled_features)\n",
        "\n",
        "print(\"\\nDriver features with cluster labels (first 5 rows):\")\n",
        "display(driver_features.head())\n",
        "\n",
        "print(\"\\nRider features with cluster labels (first 5 rows):\")\n",
        "display(rider_features.head())\n",
        "print(\"\\nDriver Cluster Analysis (Mean of Features per Cluster):\")\n",
        "display(driver_features.groupby('driver_cluster')[driver_clustering_features.columns].mean())\n",
        "\n",
        "print(\"\\nRider Cluster Analysis (Mean of Features per Cluster):\")\n",
        "display(rider_features.groupby('rider_cluster')[rider_clustering_features.columns].mean())\n",
        "dfs['search_data']['search_request_created_at'] = pd.to_datetime(dfs['search_data']['search_request_created_at'])\n",
        "dfs['quote_data']['quote_created_at'] = pd.to_datetime(dfs['quote_data']['quote_created_at'])\n",
        "dfs['booking_data']['booking_created_at'] = pd.to_datetime(dfs['booking_data']['booking_created_at'])\n",
        "dfs['booking_cancellation_data']['cancelled_at'] = pd.to_datetime(dfs['booking_cancellation_data']['cancelled_at'])\n",
        "\n",
        "weekly_searches = dfs['search_data'].set_index('search_request_created_at').resample('W')['search_request_id'].nunique()\n",
        "weekly_quotes = dfs['quote_data'].set_index('quote_created_at').resample('W')['quote_id'].nunique()\n",
        "weekly_bookings = dfs['booking_data'].set_index('booking_created_at').resample('W')['booking_id'].nunique()\n",
        "weekly_cancellations = dfs['booking_cancellation_data'].set_index('cancelled_at').resample('W')['booking_id'].nunique()\n",
        "\n",
        "print(\"Weekly searches (first 5 weeks):\")\n",
        "display(weekly_searches.head())\n",
        "print(\"\\nWeekly quotes (first 5 weeks):\")\n",
        "display(weekly_quotes.head())\n",
        "print(\"\\nWeekly bookings (first 5 weeks):\")\n",
        "display(weekly_bookings.head())\n",
        "print(\"\\nWeekly cancellations (first 5 weeks):\")\n",
        "display(weekly_cancellations.head())\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weekly_search_to_quote_rate = (weekly_quotes / weekly_searches) * 100\n",
        "weekly_quote_to_booking_rate = (weekly_bookings / weekly_quotes) * 100\n",
        "\n",
        "weekly_cancellation_rate = (weekly_cancellations / weekly_bookings) * 100\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "plt.plot(weekly_search_to_quote_rate.index, weekly_search_to_quote_rate.values, marker='o', linestyle='-', label='Search to Quote Conversion Rate')\n",
        "plt.plot(weekly_quote_to_booking_rate.index, weekly_quote_to_booking_rate.values, marker='o', linestyle='-', label='Quote to Booking Conversion Rate')\n",
        "plt.plot(weekly_cancellation_rate.index, weekly_cancellation_rate.values, marker='o', linestyle='-', label='Overall Cancellation Rate')\n",
        "\n",
        "plt.title('Weekly Trends of Conversion and Cancellation Rates')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Rate (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UL2do7PODmDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# question 4\n",
        "## Q4. Funnel Visualization with Storytelling Build Plotly funnel charts: an overall funnel and a segmented funnel (e.g., by trip length or driver rating). Use color gradients to highlight drop severity and add annotations for key insights. Write a detailed summary interpreting the funnel visuals."
      ],
      "metadata": {
        "id": "Aey3doR4D6fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_search_requests = dfs['search_data']['search_request_id'].nunique()\n",
        "num_quotes = dfs['quote_data']['quote_id'].nunique()\n",
        "num_bookings = dfs['booking_data']['booking_id'].nunique()\n",
        "num_completed_bookings = dfs['booking_data'][dfs['booking_data']['status'] == 'COMPLETED']['booking_id'].nunique()\n",
        "\n",
        "overall_funnel_df = pd.DataFrame({\n",
        "    'Stage': ['Search Requests', 'Quotes', 'Bookings', 'Completed Bookings'],\n",
        "    'Count': [num_search_requests, num_quotes, num_bookings, num_completed_bookings]\n",
        "})\n",
        "\n",
        "print(\"Overall Booking Funnel:\")\n",
        "display(overall_funnel_df)\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(go.Funnel(\n",
        "    y = overall_funnel_df['Stage'],\n",
        "    x = overall_funnel_df['Count'],\n",
        "    textinfo = \"value+percent initial\",\n",
        "    marker = {\"color\": [\"deepskyblue\", \"lightsalmon\", \"tan\", \"teal\"]},\n",
        "    connector = {\"line\": {\"color\": \"royalblue\", \"dash\": \"dot\", \"width\": 3}}\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text='Overall Booking Funnel',\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "booking_quote_merge = pd.merge(dfs['booking_data'], dfs['quote_data'][['quote_id', 'search_request_id']], on='quote_id', how='left')\n",
        "full_funnel_data = pd.merge(booking_quote_merge, dfs['search_data'][['search_request_id', 'estimated_distance']], on='search_request_id', how='left')\n",
        "\n",
        "full_funnel_data['stage'] = 'Bookings'\n",
        "full_funnel_data.loc[full_funnel_data['status'] == 'COMPLETED', 'stage'] = 'Completed Bookings'\n",
        "\n",
        "searches_only = dfs['search_data'][~dfs['search_data']['search_request_id'].isin(full_funnel_data['search_request_id'])]\n",
        "searches_only['stage'] = 'Search Requests'\n",
        "searches_only['booking_id'] = None\n",
        "searches_only['quote_id'] = None\n",
        "\n",
        "\n",
        "quotes_only = dfs['quote_data'][~dfs['quote_data']['quote_id'].isin(full_funnel_data['quote_id'])]\n",
        "quotes_only = pd.merge(quotes_only, dfs['search_data'][['search_request_id', 'estimated_distance']], on='search_request_id', how='left')\n",
        "quotes_only['stage'] = 'Quotes'\n",
        "quotes_only['booking_id'] = None\n",
        "\n",
        "full_funnel_data_subset = full_funnel_data[['search_request_id', 'estimated_distance', 'stage', 'booking_id', 'quote_id']]\n",
        "searches_only_subset = searches_only[['search_request_id', 'estimated_distance', 'stage', 'booking_id', 'quote_id']]\n",
        "quotes_only_subset = quotes_only[['search_request_id', 'estimated_distance', 'stage', 'booking_id', 'quote_id']]\n",
        "\n",
        "\n",
        "segmented_funnel_data = pd.concat([full_funnel_data_subset, searches_only_subset, quotes_only_subset])\n",
        "\n",
        "trip_distance_bins = [0, 5000, 10000, 25000, 50000, 100000, float('inf')]\n",
        "trip_distance_labels = ['0-5km', '5-10km', '10-25km', '25-50km', '50-100km', '>100km']\n",
        "segmented_funnel_data['trip_distance_bucket'] = pd.cut(segmented_funnel_data['estimated_distance'], bins=trip_distance_bins, labels=trip_distance_labels, right=False)\n",
        "\n",
        "stage_order = ['Search Requests', 'Quotes', 'Bookings', 'Completed Bookings']\n",
        "\n",
        "\n",
        "segmented_funnel_counts_list = []\n",
        "\n",
        "for bucket in trip_distance_labels:\n",
        "    bucket_data = segmented_funnel_data[segmented_funnel_data['trip_distance_bucket'] == bucket]\n",
        "    for stage in stage_order:\n",
        "        count = 0\n",
        "        if stage == 'Search Requests':\n",
        "            count = bucket_data[bucket_data['stage'] == stage]['search_request_id'].nunique()\n",
        "        elif stage == 'Quotes':\n",
        "            count = bucket_data[bucket_data['stage'] == stage]['quote_id'].nunique()\n",
        "        elif stage == 'Bookings' or stage == 'Completed Bookings':\n",
        "             count = bucket_data[bucket_data['stage'] == stage]['booking_id'].nunique()\n",
        "\n",
        "        segmented_funnel_counts_list.append({'trip_distance_bucket': bucket, 'stage': stage, 'count': count})\n",
        "\n",
        "segmented_funnel_counts = pd.DataFrame(segmented_funnel_counts_list)\n",
        "\n",
        "segmented_funnel_counts['stage'] = pd.Categorical(segmented_funnel_counts['stage'], categories=stage_order, ordered=True)\n",
        "segmented_funnel_counts = segmented_funnel_counts.sort_values(['trip_distance_bucket', 'stage'])\n",
        "\n",
        "print(\"Segmented Funnel Data by Trip Distance Bucket:\")\n",
        "display(segmented_funnel_counts.head())\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "funnel_charts = []\n",
        "\n",
        "for bucket in segmented_funnel_counts['trip_distance_bucket'].unique():\n",
        "    bucket_data = segmented_funnel_counts[segmented_funnel_counts['trip_distance_bucket'] == bucket]\n",
        "\n",
        "    funnel = go.Funnel(\n",
        "        name=bucket,\n",
        "        y=bucket_data['stage'],\n",
        "        x=bucket_data['count'],\n",
        "        textinfo=\"value+percent initial\",\n",
        "        marker={\"color\": [\"deepskyblue\", \"lightsalmon\", \"tan\", \"teal\"]},\n",
        "        connector={\"line\": {\"color\": \"royalblue\", \"dash\": \"dot\", \"width\": 3}}\n",
        "    )\n",
        "\n",
        "    funnel_charts.append(funnel)\n",
        "\n",
        "fig = go.Figure(data=funnel_charts)\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text='Segmented Booking Funnel by Trip Distance',\n",
        "    height=600,\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "mNP_5ODTEBzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}